{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1964f2d4-7506-4088-a4cc-cd9d7bce25fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71889e3-6ae0-41fd-b7cf-0035325ffbfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.formatCheck.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3dd0cbf-0bd4-4f82-8427-03457bb25c7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/UberPickUps/bronze_tables/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed81946f-a4ad-40ea-94f0-1e73cd74b23e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'bronze_tables' created successfully at path 'dbfs:/FileStore/UberPickUps/bronze_tables/'.\n"
     ]
    }
   ],
   "source": [
    "new_folder_name = \"bronze_tables\"\n",
    "new_folder_path = \"dbfs:/FileStore/UberPickUps/\" + new_folder_name + \"/\"\n",
    "dbutils.fs.mkdirs(new_folder_path)\n",
    "\n",
    "try:\n",
    "    dbutils.fs.mkdirs(new_folder_path)\n",
    "    print(f\"Folder '{new_folder_name}' created successfully at path '{new_folder_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create folder '{new_folder_name}' at path '{new_folder_path}'.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6821728-091d-4ffa-9417-4962eefc4a7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: []"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"dbfs:/FileStore/UberPickUps/bronze_tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb9d088-0374-4e34-8889-a31449bdffc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_column_name = 'Time'\n",
    "new_schema_type = StringType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51d1f3c-f23f-4111-8c6e-5521bfd4cbaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "create_bronze = f'CREATE DATABASE IF NOT EXISTS BRONZE'\n",
    "spark.sql(create_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb91374-4684-45de-b5db-5ccc6be60b35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/Uber_Jan_Feb_FOIL.csv'\n",
    "table_name = 'Uber_Jan_Feb_FOIL'\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(csv_path)\n",
    "df1 = df1.withColumn(\"Category\", lit(table_name))\n",
    "df1.write.format(\"delta\").mode(\"overwrite\").option(\"path\", new_folder_path).option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n",
    "table = \"bronze.\" + table_name\n",
    "df1.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2790315f-dc73-4e5c-80cb-c5bdad817b8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "  csv_path = '/FileStore/tables/Uber Pick Ups/other_American_B01362.csv'\n",
    "table_name = 'other_American_B01362'\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(csv_path)\n",
    "df2 = df2.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "if \"TIME\" in df2.columns:\n",
    "    df2 = df2.withColumnRenamed(\"TIME\", new_column_name).withColumn(new_column_name, col(new_column_name).cast(new_schema_type))\n",
    "    \n",
    "    try:\n",
    "        df2.write.mode(\"overwrite\").option(\"path\", new_folder_path).option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "else:\n",
    "    print(f\"No column named 'TIME' found in table {table_name}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df2.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e441fbf5-e7b0-4215-9994-d94e2963a444",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/other_Carmel_B00256.csv'\n",
    "table_name = 'other_Carmel_B00256'\n",
    "df3 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df3 = df3.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "try:\n",
    "    df3.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df3.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c517ca-be45-4e73-a2ac-3de5ce36ca70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/other_Diplo_B01196.csv'\n",
    "table_name = 'other_Diplo_B01196'\n",
    "df4 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df4 = df4.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "if \"Time\" in df4.columns:\n",
    "    df4 = df4.withColumnRenamed(\"Time\", new_column_name).withColumn(new_column_name, col(new_column_name).cast(new_schema_type))\n",
    "    \n",
    "    try:\n",
    "        df4.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "else:\n",
    "    print(f\"No column named 'TIME' found in table {table_name}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df4.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4864c281-dd1d-4b44-bc53-74270f219484",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/other_FHV_services_jan_aug_2015.csv'\n",
    "table_name = 'other_FHV_services_jan_aug_2015'\n",
    "df5 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df5 = df5.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "try:\n",
    "    df5.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df5.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cde05ce-f254-4c81-8453-ae49943a2d5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/other_Federal_02216.csv'\n",
    "table_name = 'other_Federal_02216'\n",
    "df6 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df6 = df6.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "if \"Time\" in df6.columns:\n",
    "    df6 = df6.withColumnRenamed(\"Time\", new_column_name).withColumn(new_column_name, col(new_column_name).cast(new_schema_type))\n",
    "    \n",
    "    try:\n",
    "        df6.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "else:\n",
    "    print(f\"No column named 'TIME' found in table {table_name}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df6.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5481653-f5fb-4ddc-81d2-ec3d4919e356",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/other_Firstclass_B01536.csv'\n",
    "table_name = 'other_Firstclass_B01536'\n",
    "df7 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df7 = df7.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "if \"TIME\" in df7.columns:\n",
    "    df7 = df7.withColumnRenamed(\"TIME\", new_column_name).withColumn(new_column_name, col(new_column_name).cast(new_schema_type))\n",
    "    \n",
    "    try:\n",
    "        df7.write.mode(\"overwrite\").option(\"path\", new_folder_path).option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "else:\n",
    "    print(f\"No column named 'TIME' found in table {table_name}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df7.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad482a8-9232-45c7-83f2-46792f5cf46f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/other_Highclass_B01717.csv'\n",
    "table_name = 'other_Highclass_B01717'\n",
    "df8 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df8 = df8.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "if \"TIME\" in df8.columns:\n",
    "    df8 = df8.withColumnRenamed(\"TIME\", new_column_name).withColumn(new_column_name, col(new_column_name).cast(new_schema_type))\n",
    "    \n",
    "    try:\n",
    "        df8.write.mode(\"overwrite\").option(\"path\", new_folder_path).option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "else:\n",
    "    print(f\"No column named 'TIME' found in table {table_name}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df8.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f5bcc5-7239-441e-8c9e-38db5c4d7ada",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/other_Lyft_B02510.csv'\n",
    "table_name = 'other_Lyft_B02510'\n",
    "df9 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df9 = df9.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "try:\n",
    "    df9.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df9.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69552143-d1ed-455b-a8ae-6ee9345dabf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/other_Prestige_B01338.csv'\n",
    "table_name = 'other_Prestige_B01338'\n",
    "df10 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df10 = df10.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "if \"TIME\" in df10.columns:\n",
    "    df10 = df10.withColumnRenamed(\"TIME\", new_column_name).withColumn(new_column_name, col(new_column_name).cast(new_schema_type))\n",
    "    \n",
    "    try:\n",
    "        df10.write.mode(\"overwrite\").option(\"path\", new_folder_path).option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "else:\n",
    "    print(f\"No column named 'TIME' found in table {table_name}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df10.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13cc2d76-2ac3-4b40-9006-63da2ccd5b98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/other_Skyline_B00111.csv'\n",
    "table_name = 'other_Skyline_B00111'\n",
    "df11 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df11 = df11.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "if \"Time\" in df11.columns:\n",
    "    df11 = df11.withColumnRenamed(\"Time\", new_column_name).withColumn(new_column_name, col(new_column_name).cast(new_schema_type))\n",
    "    \n",
    "    try:\n",
    "        df11.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "else:\n",
    "    print(f\"No column named 'TIME' found in table {table_name}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df11.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e529d16-82b2-4e03-9162-8c3a2ef4549a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/uber_raw_data_apr14.csv'\n",
    "table_name = 'uber_raw_data_apr14'\n",
    "df12 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df12 = df12.withColumn(\"Category\", lit(table_name))\n",
    "try:\n",
    "    df12.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df12.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b95aa673-904b-4a0e-add9-ddf9947e7ad1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/uber_raw_data_aug14.csv'\n",
    "table_name = 'uber_raw_data_aug14'\n",
    "df13 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df13 = df13.withColumn(\"Category\", lit(table_name))\n",
    "try:\n",
    "    df13.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df13.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad9f6c1c-f287-40f8-84bb-cd0340bd2766",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/uber_raw_data_janjune_15.csv'\n",
    "table_name = 'uber_raw_data_janjune_15'\n",
    "df14 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df14 = df14.withColumn(\"Category\", lit(table_name))\n",
    "\n",
    "try:\n",
    "    df14.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df14.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca79145-7e03-48ff-9922-c0418e5e38df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/uber_raw_data_jul14.csv'\n",
    "table_name = 'uber_raw_data_jul14'\n",
    "df15 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df15 = df15.withColumn(\"Category\", lit(table_name))\n",
    "try:\n",
    "    df15.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df15.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "421cf20b-711f-494e-bdad-9bb5f883b6e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/uber_raw_data_may14.csv'\n",
    "table_name = 'uber_raw_data_may14'\n",
    "df16 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df16 = df16.withColumn(\"Category\", lit(table_name))\n",
    "try:\n",
    "    df16.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df16.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95211ec5-3c61-4397-9a2a-067fed9ab724",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/uber_raw_data_sep14.csv'\n",
    "table_name = 'uber_raw_data_sep14'\n",
    "df17 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df17 = df17.withColumn(\"Category\", lit(table_name))\n",
    "try:\n",
    "    df17.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df17.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a02f917-7eee-476d-9b1e-e505fab636e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/FileStore/tables/Uber Pick Ups/uber_raw_data_jun14.csv'\n",
    "table_name = 'uber_raw_data_jun14'\n",
    "df18 = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df18 = df18.withColumn(\"Category\", lit(table_name))\n",
    "try:\n",
    "    df18.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"path\", new_folder_path).saveAsTable(table_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while saving table {table_name}: {str(e)}\")\n",
    "table = \"bronze.\" + table_name\n",
    "df18.write.mode(\"overwrite\").saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd245a8-ce76-4578-9eae-c7fba1090709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[40]: [FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/_delta_log/', name='_delta_log/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-0131bf55-5c04-4831-84ca-e2ea4680ed5a-c000.snappy.parquet', name='part-00000-0131bf55-5c04-4831-84ca-e2ea4680ed5a-c000.snappy.parquet', size=4053, modificationTime=1715742593000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-070de65d-6db2-4448-af6a-c58a0af94205-c000.snappy.parquet', name='part-00000-070de65d-6db2-4448-af6a-c58a0af94205-c000.snappy.parquet', size=3998, modificationTime=1715742621000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-1c9ce1f2-fe57-4a29-8267-ca07c8b8182a-c000.snappy.parquet', name='part-00000-1c9ce1f2-fe57-4a29-8267-ca07c8b8182a-c000.snappy.parquet', size=3675, modificationTime=1715742562000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-2620d1a1-255a-42a0-960a-d84c4770e0ed-c000.snappy.parquet', name='part-00000-2620d1a1-255a-42a0-960a-d84c4770e0ed-c000.snappy.parquet', size=3622, modificationTime=1715742534000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-30eb4602-e800-47f9-9806-067a7eca3f5a-c000.snappy.parquet', name='part-00000-30eb4602-e800-47f9-9806-067a7eca3f5a-c000.snappy.parquet', size=4070, modificationTime=1715742572000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-42b2797d-29a6-4c03-a1bb-14e4b92045ca-c000.snappy.parquet', name='part-00000-42b2797d-29a6-4c03-a1bb-14e4b92045ca-c000.snappy.parquet', size=3772, modificationTime=1715742540000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-57d6681f-6bb1-484e-bd37-607f20ad8ad3-c000.snappy.parquet', name='part-00000-57d6681f-6bb1-484e-bd37-607f20ad8ad3-c000.snappy.parquet', size=4003, modificationTime=1715742642000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-838d41b9-7c94-4460-9788-379c1386c5a8-c000.snappy.parquet', name='part-00000-838d41b9-7c94-4460-9788-379c1386c5a8-c000.snappy.parquet', size=3839, modificationTime=1715742546000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-84be1a23-dc72-4be2-b86b-17e541612924-c000.snappy.parquet', name='part-00000-84be1a23-dc72-4be2-b86b-17e541612924-c000.snappy.parquet', size=3968, modificationTime=1715742629000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-aff5eaa6-d9e4-44d4-a51f-339d882b27bd-c000.snappy.parquet', name='part-00000-aff5eaa6-d9e4-44d4-a51f-339d882b27bd-c000.snappy.parquet', size=2875, modificationTime=1715742526000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-c4a080c8-0cf6-4dac-8dbf-c308f523fcc6-c000.snappy.parquet', name='part-00000-c4a080c8-0cf6-4dac-8dbf-c308f523fcc6-c000.snappy.parquet', size=4751, modificationTime=1715742552000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-c4d444c1-3015-4213-b450-35443fba2656-c000.snappy.parquet', name='part-00000-c4d444c1-3015-4213-b450-35443fba2656-c000.snappy.parquet', size=4053, modificationTime=1715742636000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-cbd63841-20e4-4304-87ab-8c110dd93c87-c000.snappy.parquet', name='part-00000-cbd63841-20e4-4304-87ab-8c110dd93c87-c000.snappy.parquet', size=3980, modificationTime=1715742567000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-e4b1ab2c-2727-47be-a314-3dba52f09ac5-c000.snappy.parquet', name='part-00000-e4b1ab2c-2727-47be-a314-3dba52f09ac5-c000.snappy.parquet', size=12567, modificationTime=1715742557000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-e7731a6d-ed1d-4724-957e-afd6166fa561-c000.snappy.parquet', name='part-00000-e7731a6d-ed1d-4724-957e-afd6166fa561-c000.snappy.parquet', size=4129, modificationTime=1715742583000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-ec828338-dca9-45da-89f9-4db1ebbd0fc3-c000.snappy.parquet', name='part-00000-ec828338-dca9-45da-89f9-4db1ebbd0fc3-c000.snappy.parquet', size=2592, modificationTime=1715742611000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-fb3f27fa-230a-48c4-9cad-ebc021b1e700-c000.snappy.parquet', name='part-00000-fb3f27fa-230a-48c4-9cad-ebc021b1e700-c000.snappy.parquet', size=4113, modificationTime=1715742577000),\n FileInfo(path='dbfs:/FileStore/UberPickUps/bronze_tables/part-00000-ff93b3e7-7bb5-41a9-9a6e-1369fc6870f7-c000.snappy.parquet', name='part-00000-ff93b3e7-7bb5-41a9-9a6e-1369fc6870f7-c000.snappy.parquet', size=3963, modificationTime=1715742603000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"dbfs:/FileStore/UberPickUps/bronze_tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c7c40b7-4c07-4dc7-b4c2-3363382e685b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Nested databases are not supported by v1 session catalog: delta.`dbfs:/FileStore/UberPickUps/bronze_tables/`.\nError reading table _delta: Incompatible format detected.\n\nYou are trying to read from `dbfs:/FileStore/UberPickUps/bronze_tables/_delta_log/` using Delta, but there is no\ntransaction log present. Check the upstream job to make sure that it is writing\nusing format(\"delta\") and that you are trying to read from the table base path.\n\nTo disable this check, SET spark.databricks.delta.formatCheck.enabled=false\nTo learn more about Delta, see https://docs.databricks.com/delta/index.html\nError reading table part-00000-179052da-8f35-4d13-981f-62d6939b1ff1-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-179052da-8f35-4d13-981f-62d6939b1ff1-c000.snappy.parquet\nError reading table part-00000-1b8f3fc8-489d-4c3e-a9a7-edf462a36823-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-1b8f3fc8-489d-4c3e-a9a7-edf462a36823-c000.snappy.parquet\nError reading table part-00000-32ebed7d-ce1c-43fb-aede-bbd82ec818d9-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-32ebed7d-ce1c-43fb-aede-bbd82ec818d9-c000.snappy.parquet\nError reading table part-00000-36fda30e-4d4d-4764-a39d-2ac5ad15ad04-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-36fda30e-4d4d-4764-a39d-2ac5ad15ad04-c000.snappy.parquet\nError reading table part-00000-3fcff9a7-62c3-4315-b71a-96ad2b19afbc-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-3fcff9a7-62c3-4315-b71a-96ad2b19afbc-c000.snappy.parquet\nError reading table part-00000-415cef58-8cfa-4096-984d-5842257fbc3f-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-415cef58-8cfa-4096-984d-5842257fbc3f-c000.snappy.parquet\nError reading table part-00000-4fb6bdf0-fcdb-47f4-8f54-03c9e52ce12d-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-4fb6bdf0-fcdb-47f4-8f54-03c9e52ce12d-c000.snappy.parquet\nError reading table part-00000-521e1b94-f655-4f82-9f17-de9f51500dd3-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-521e1b94-f655-4f82-9f17-de9f51500dd3-c000.snappy.parquet\nError reading table part-00000-52f9c3e3-8b55-4582-a40b-f5157b6bbeb0-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-52f9c3e3-8b55-4582-a40b-f5157b6bbeb0-c000.snappy.parquet\nError reading table part-00000-62127639-de3d-485c-b050-486236f9cbb6-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-62127639-de3d-485c-b050-486236f9cbb6-c000.snappy.parquet\nError reading table part-00000-952d2604-53bc-4e44-aab1-ca18c12add05-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-952d2604-53bc-4e44-aab1-ca18c12add05-c000.snappy.parquet\nError reading table part-00000-9f58d1f1-bc63-43b8-a3a5-3d3ff46c1db1-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-9f58d1f1-bc63-43b8-a3a5-3d3ff46c1db1-c000.snappy.parquet\nError reading table part-00000-b3260a9c-f69d-4f48-bb2b-4f3adce284b1-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-b3260a9c-f69d-4f48-bb2b-4f3adce284b1-c000.snappy.parquet\nError reading table part-00000-d5f2cd96-10e4-45ca-90c7-d9e91f6f5f3f-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-d5f2cd96-10e4-45ca-90c7-d9e91f6f5f3f-c000.snappy.parquet\nError reading table part-00000-f17dfc2f-0c6f-4650-9d88-e2866dd57719-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-f17dfc2f-0c6f-4650-9d88-e2866dd57719-c000.snappy.parquet\nError reading table part-00000-f1d6b705-5aaf-486c-9d9d-0aef359b6b57-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-f1d6b705-5aaf-486c-9d9d-0aef359b6b57-c000.snappy.parquet\nError reading table part-00000-f8780145-8f4a-496b-900a-aed9ba9b56cd-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-f8780145-8f4a-496b-900a-aed9ba9b56cd-c000.snappy.parquet\nError reading table part-00000-ffc4c989-fe3a-4fdc-be45-bbde134b58bf-c000.snappy.pa: A partition path fragment should be the form like `part1=foo/part2=bar`. The partition path: part-00000-ffc4c989-fe3a-4fdc-be45-bbde134b58bf-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "delta_table_path = 'dbfs:/FileStore/UberPickUps/bronze_tables/'\n",
    "\n",
    "# List all Delta files within the directory\n",
    "try:\n",
    "    table_names = spark.sql(\"SHOW TABLES IN delta.`{}`\".format(delta_table_path)).toPandas()\n",
    "    table_names = table_names['tableName'].tolist()\n",
    "except AnalysisException as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    table_names = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    table_name = file_path.name[:-5]  # Remove the \".delta\" extension\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(file_path.path)\n",
    "        print(f\"Table: {table_name}\")\n",
    "        df.show(5) \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading table {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c363d2bc-585d-4b5c-9e91-32bcb18afddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesting to bronze",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
